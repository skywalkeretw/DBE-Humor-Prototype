{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning using NLP and bert\n",
    "## Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_csv_into_dir(csv_name, label, feature):\n",
    "    \"\"\"\n",
    "    split_csv_into_dir takes a csv file and splits its content by the label. The two new data sets pos and neg are split into a train and test set for each. These are now stored in respective directories \n",
    "\n",
    "    :param csv_name: name or url of the csv contining the data\n",
    "    :param label: the label column inside the csv is expected to be bool\n",
    "    :param feature: the feature column inside the csv is expected to be string\n",
    "    \"\"\" \n",
    "\n",
    "    # Read in dataset\n",
    "    data = pd.read_csv(csv_name)\n",
    "    total_data = len(data)\n",
    "    print(\"---\")\n",
    "    print(\"Data has been read in\")\n",
    "    print(\"Columns:\\n\")\n",
    "    print(data.columns)\n",
    "    print(\"Head()\\n\")\n",
    "    print(data.head())\n",
    "    print(\"Info()\\n\")\n",
    "    print(data.info())\n",
    "    print(\"Describe()\\n\")\n",
    "    print(data.describe())\n",
    "    print(\"There are\", total_data, \"entries\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # Use mask to split list into pos and neg\n",
    "    mask = data[label] == True\n",
    "    pos, neg = data[mask], data[~mask]\n",
    "    print(\"---\")\n",
    "    print(\"Data has been split into True and False\\n\")\n",
    "    print(\"pos len:\", len(pos))\n",
    "    print(pos.head())\n",
    "    print(\"\\n-\\n\")\n",
    "    print(\"neg len:\", len(neg))\n",
    "    print(neg.head())\n",
    "    print(\"\")\n",
    "    \n",
    "    # Split pos data and neg into train and test set\n",
    "    data = {'train': {'pos': [],'neg': [],},'test': {'pos': [],'neg': [],}}\n",
    "    data['train']['pos'], data['test']['pos'] = train_test_split(pos, test_size=0.2)\n",
    "    data['train']['neg'], data['test']['neg'] = train_test_split(neg, test_size=0.2)\n",
    "    print(\"---\")\n",
    "    print(\"Datasets Have been Split into train and test\")\n",
    "    print(\"\")    \n",
    "\n",
    "    # Create Folder structure and save files into it \n",
    "    # Code fails if folders already exist\n",
    "    print(\"---\")\n",
    "    train_test_dirs = ['test', 'train']\n",
    "    pos_neg_dirs = ['pos', 'neg']\n",
    "    for train_test_dir in train_test_dirs:\n",
    "        try:\n",
    "            os.mkdir(train_test_dir)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "\n",
    "        \n",
    "        for pos_neg_dir in pos_neg_dirs:\n",
    "            new_dir = os.path.join(train_test_dir, pos_neg_dir)\n",
    "            try:\n",
    "                os.mkdir(new_dir)\n",
    "            except FileExistsError:\n",
    "                print(\"Cleaning Dir:\", new_dir)\n",
    "                shutil.rmtree(new_dir)\n",
    "                os.mkdir(new_dir)\n",
    "                pass\n",
    "            print(\"Dir\", new_dir, \"has been created\")\n",
    "            \n",
    "            data_len = len(data[train_test_dir][pos_neg_dir])\n",
    "            \n",
    "            for idx, row in data[train_test_dir][pos_neg_dir].reset_index(drop=True).iterrows():\n",
    "                #create new file named after current index\n",
    "                with open(os.path.join(new_dir, str(idx)+\".txt\"), \"w\") as file:\n",
    "                    # write the text into the new file\n",
    "                    file.write(row[feature])\n",
    "            \n",
    "            folder_content_len = 0\n",
    "            for path in os.scandir(new_dir):\n",
    "                if path.is_file():\n",
    "                    folder_content_len += 1\n",
    "            if data_len != folder_content_len:\n",
    "                print('Dir ', new_dir ,'is missing Jokes', \"Data:\", data_len, \"Dir Content\", folder_content_len)\n",
    "            else:\n",
    "                print(\"All files have been Created!\")\n",
    "    print(\"Success\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model(tfhub_handle_preprocess, tfhub_handle_encoder, var_layer_dropout, var_activation_funct):\n",
    "    \"\"\"\n",
    "    build_classifier_model returns teh keras model.\n",
    "\n",
    "    :param tfhub_handle_preprocess: \n",
    "    :param tfhub_handle_encoder: \n",
    "    :param var_layer_dropout: \n",
    "    :param var_activation_funct: \n",
    "    :return: Keras Model\n",
    "    \"\"\" \n",
    "    text_input = tf.keras.layers.Input(shape=(),dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    \n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    output = encoder(encoder_inputs)\n",
    "    print(output)\n",
    "    net = output['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(var_layer_dropout)(net) # HYPERPARAMETER DROPOUT\n",
    "    net = tf.keras.layers.Dense(1, activation=var_activation_funct, name='classifier')(net) # HYPERPARAMETER Layer Size (1) und Activation\n",
    "    \n",
    "    print(\"Return Model\")\n",
    "    return tf.keras.Model(text_input, net)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_funny(inputs, results):\n",
    "  result_for_printing = [f'{inputs[i]:<30}\\n FUNNY SCORE: {results[i][0]:.6f}\\n' for i in range(len(inputs))]\n",
    "  print(result_for_printing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: change url\n",
    "csv_name = \"https://raw.githubusercontent.com/skywalkeretw/DBE-Humor-Prototype/master/JokeAPI/dataset.csv\"\n",
    "label = 'humor'\n",
    "feature = 'text'\n",
    "\n",
    "split_csv_into_dir(csv_name, label, feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_batch_size = 512\n",
    "var_seed = 42\n",
    "var_layer_dropout = 0.1 \n",
    "var_activation_funct = None # sigmoid, relu\n",
    "var_epochs = 2\n",
    "var_init_lr = 3e-5\n",
    "var_optimizer = 'adamw'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get The Date from the folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = var_batch_size # HYPERPARAMETER\n",
    "seed = var_seed # HYPERPARAMETER\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "class_names = raw_train_ds.class_names\n",
    "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'test',\n",
    "    batch_size=batch_size)\n",
    "\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bert models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.8 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "bert_models = pd.read_json('Datasets/bert_models.json').to_dict('records')\n",
    "bert_models = [\n",
    "   {\n",
    "      \"name\":\"small_bert/bert_en_uncased_L-2_H-128_A-2\",\n",
    "      \"encoder\":\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"small_bert/bert_en_uncased_L-2_H-256_A-4\",\n",
    "      \"encoder\":\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"small_bert/bert_en_uncased_L-2_H-512_A-8\",\n",
    "      \"encoder\":\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"small_bert/bert_en_uncased_L-2_H-768_A-12\",\n",
    "      \"encoder\":\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"small_bert/bert_en_uncased_L-4_H-128_A-2\",\n",
    "      \"encoder\":\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"small_bert/bert_en_uncased_L-4_H-256_A-4\",\n",
    "      \"encoder\":\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"small_bert/bert_en_uncased_L-4_H-512_A-8\",\n",
    "      \"encoder\":\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"small_bert/bert_en_uncased_L-4_H-768_A-12\",\n",
    "      \"encoder\":\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"small_bert/bert_en_uncased_L-6_H-128_A-2\",\n",
    "      \"encoder\":\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"small_bert/bert_en_uncased_L-6_H-256_A-4\",\n",
    "      \"encoder\":\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"small_bert/bert_en_uncased_L-6_H-512_A-8\",\n",
    "      \"encoder\":\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"small_bert/bert_en_uncased_L-6_H-768_A-12\",\n",
    "      \"encoder\":\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"small_bert/bert_en_uncased_L-8_H-128_A-2\",\n",
    "      \"encoder\":\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"small_bert/bert_en_uncased_L-8_H-256_A-4\",\n",
    "      \"encoder\":\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"small_bert/bert_en_uncased_L-8_H-512_A-8\",\n",
    "      \"encoder\":\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"small_bert/bert_en_uncased_L-8_H-768_A-12\",\n",
    "      \"encoder\":\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"small_bert/bert_en_uncased_L-10_H-128_A-2\",\n",
    "      \"encoder\":\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"small_bert/bert_en_uncased_L-10_H-256_A-4\",\n",
    "      \"encoder\":\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"small_bert/bert_en_uncased_L-10_H-512_A-8\",\n",
    "      \"encoder\":\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"small_bert/bert_en_uncased_L-10_H-768_A-12\",\n",
    "      \"encoder\":\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"small_bert/bert_en_uncased_L-12_H-128_A-2\",\n",
    "      \"encoder\":\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"small_bert/bert_en_uncased_L-12_H-256_A-4\",\n",
    "      \"encoder\":\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"small_bert/bert_en_uncased_L-12_H-512_A-8\",\n",
    "      \"encoder\":\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"small_bert/bert_en_uncased_L-12_H-768_A-12\",\n",
    "      \"encoder\":\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"albert_en_base\",\n",
    "      \"encoder\":\"https://tfhub.dev/tensorflow/albert_en_base/2\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/albert_en_preprocess/3\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"electra_small\",\n",
    "      \"encoder\":\"https://tfhub.dev/google/electra_small/2\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"electra_base\",\n",
    "      \"encoder\":\"https://tfhub.dev/google/electra_base/2\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"bert_en_uncased_L-12_H-768_A-12\",\n",
    "      \"encoder\":\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"bert_en_cased_L-12_H-768_A-12\",\n",
    "      \"encoder\":\"https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"bert_multi_cased_L-12_H-768_A-12\",\n",
    "      \"encoder\":\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3\",\n",
    "      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3\"\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in bert_models:\n",
    "    bert_model_name = model.name\n",
    "    \n",
    "    tfhub_handle_encoder = model.encoder\n",
    "    tfhub_handle_preprocess = model.preprocess\n",
    "\n",
    "    bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "\n",
    "    bert_model = hub.KerasLayer(tfhub_handle_encoder)\n",
    "    \n",
    "    classifier_model = build_classifier_model()\n",
    "\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    metrics = tf.metrics.BinaryAccuracy()\n",
    "\n",
    "    epochs = var_epochs # HYPERPARAMETER\n",
    "    steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "    num_train_steps = steps_per_epoch * epochs\n",
    "    num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "    init_lr = var_init_lr # HYPERPARAMETER\n",
    "    optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                              num_train_steps=num_train_steps,\n",
    "                                              num_warmup_steps=num_warmup_steps,\n",
    "                                              optimizer_type=var_optimizer)\n",
    "\n",
    "    classifier_model.compile(optimizer=optimizer,\n",
    "                             loss=loss,\n",
    "                             metrics=metrics)\n",
    "\n",
    "    print(f'Training of {bert_model_name}')\n",
    "    history = classifier_model.fit(x=train_ds,\n",
    "                                   validation_data=val_ds,\n",
    "                                   epochs=epochs)\n",
    "\n",
    "    loss, accuracy = classifier_model.evaluate(test_ds)\n",
    "\n",
    "    print(f'Loss {bert_model_name}: {loss}')\n",
    "    print(f'Accuracy {bert_model_name}: {accuracy}')\n",
    "    \n",
    "    history_dict = history.history\n",
    "    #print(history_dict.keys())\n",
    "\n",
    "    acc = history_dict['binary_accuracy']\n",
    "    val_acc = history_dict['val_binary_accuracy']\n",
    "    loss = history_dict['loss']\n",
    "    val_loss = history_dict['val_loss']\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    # r is for \"solid red line\"\n",
    "    plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "    # b is for \"solid blue line\"\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title(f'Training and validation loss {bert_model_name}')\n",
    "    # plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title(f'Training and validation accuracy {bert_model_name}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    import time\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    dataset_name = \"model_\"+bert_model_name+\"_\"+timestamp\n",
    "    saved_model_path = './{}'.format(dataset_name.replace('/', '_'))\n",
    "    print(f'Saving of {bert_model_name}')\n",
    "    classifier_model.save(saved_model_path, include_optimizer=True)\n",
    "    print()\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models and Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo add url to ourExamples.csv\n",
    "df = pd.read_csv()\n",
    "directories_in_curdir = sorted(list(filter(os.path.isdir, os.listdir(os.curdir))))\n",
    "available_models = [i for i in directories_in_curdir if i.startswith('model_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test using models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in available_models:\n",
    "    \n",
    "    reloaded_model = tf.saved_model.load(model)\n",
    "    reloaded_results = tf.sigmoid(reloaded_model(tf.constant(df['Sentence'])))\n",
    "\n",
    "    is_funny(df['Sentence'], reloaded_results)\n",
    "    \n",
    "    df[model] = reloaded_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"Models\": available_models})\n",
    "\n",
    "for model in available_models:\n",
    "    df1.loc[df1[\"Models\"] == model,\"Quality KPI (closer to 5 the best)\"] = df[model].sum()\n",
    "    df1['Delta to 5'] = df1['Quality KPI (closer to 5 the best)'] - 5\n",
    "\n",
    "df1.sort_values(by='Delta to 5', ascending=True, inplace=True)\n",
    "df1 = df1.reset_index(drop=True)\n",
    "df1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
