{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import zipfile \n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "import ibm_boto3\n",
    "from ibm_botocore.client import Config, ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG=True\n",
    "def debug(d, override = False):\n",
    "    \"\"\"\n",
    "    debug simple mode to enable and disable output\n",
    "\n",
    "    :param d: data that should be displayed\n",
    "    \"\"\" \n",
    "    if DEBUG or override:\n",
    "        print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Constants for IBM COS values\n",
    "COS_ENDPOINT = \"https://\"+\"s3.eu-de.cloud-object-storage.appdomain.cloud\" # Current list avaiable at https://control.cloud-object-storage.cloud.ibm.com/v2/endpoints\n",
    "COS_API_KEY_ID = \"BlM-T642m8cZgAbXXjYVIPMioVzB8p4HbajYCSqDqmG5\" # eg \"W00YixxxxxxxxxxMB-odB-2ySfTrFBIQQWanc--P3byk\"\n",
    "COS_INSTANCE_CRN = \"crn:v1:bluemix:public:cloud-object-storage:global:a/b9532d486ede4090ab77d390ddfdeaab:f17bd93d-ed70-40d5-8c3d-d1e006d5b6f4::\" # eg \"crn:v1:bluemix:public:cloud-object-storage:global:a/3bf0d9003xxxxxxxxxx1c3e97696b71c:d6f04d83-6c4f-4a62-a165-696756d63903::\"\n",
    "\n",
    "# Create resource\n",
    "cos_resource = ibm_boto3.resource(\"s3\",\n",
    "    ibm_api_key_id=COS_API_KEY_ID,\n",
    "    ibm_service_instance_id=COS_INSTANCE_CRN,\n",
    "    config=Config(signature_version=\"oauth\"),\n",
    "    endpoint_url=COS_ENDPOINT\n",
    ")\n",
    "\n",
    "# Create client \n",
    "cos_client = ibm_boto3.client(\"s3\",\n",
    "    ibm_api_key_id=COS_API_KEY_ID,\n",
    "    ibm_service_instance_id=COS_INSTANCE_CRN,\n",
    "    config=Config(signature_version=\"oauth\"),\n",
    "    endpoint_url=COS_ENDPOINT\n",
    ")\n",
    "# \"eu-de-standard\"\n",
    "def create_bucket(bucket_name, location):\n",
    "    print(\"Creating new bucket: {0}\".format(bucket_name))\n",
    "    try:\n",
    "        cos_resource.Bucket(bucket_name).create(\n",
    "            CreateBucketConfiguration={\n",
    "                \"LocationConstraint\": location\n",
    "            }\n",
    "        )\n",
    "        print(\"Bucket: {0} created!\".format(bucket_name))\n",
    "    except ClientError as be:\n",
    "        print(\"CLIENT ERROR: {0}\\n\".format(be))\n",
    "    except Exception as e:\n",
    "        print(\"Unable to create bucket: {0}\".format(e))\n",
    "\n",
    "def get_buckets():\n",
    "    print(\"Retrieving list of buckets\")\n",
    "    try:\n",
    "        buckets = cos_resource.buckets.all()\n",
    "        for bucket in buckets:\n",
    "            print(\"Bucket Name: {0}\".format(bucket.name))\n",
    "    except ClientError as be:\n",
    "        print(\"CLIENT ERROR: {0}\\n\".format(be))\n",
    "    except Exception as e:\n",
    "        print(\"Unable to retrieve list buckets: {0}\".format(e))\n",
    "\n",
    "def upload_file(file, bucket_name):\n",
    "    print(\"Uploading {0} to bucket  {1}\".format(file, bucket_name))\n",
    "    try:\n",
    "        key = os.path.basename(file)\n",
    "        cos_client.upload_file(file, bucket_name, key)\n",
    "        print(\"File {0} uploaded to {1}\".format(file, bucket_name))\n",
    "        return key\n",
    "    except ClientError as be:\n",
    "        print(\"CLIENT ERROR: {0}\\n\".format(be))\n",
    "    except Exception as e:\n",
    "        print(\"Unable to upload: {0}\".format(e))\n",
    "\n",
    "def download_file(bucket_name, key, file):\n",
    "    print(\"Downloading {0} from bucket  {1}\".format(key, bucket_name))\n",
    "    try:\n",
    "        cos_client.download_file(Bucket=bucket_name, Key=key,  Filename=file)\n",
    "        print(\"File {0} downloaded from {1}\".format(file, bucket_name))\n",
    "    except ClientError as be:\n",
    "        print(\"CLIENT ERROR: {0}\\n\".format(be))\n",
    "    except Exception as e:\n",
    "        print(\"Unable to download: {0}\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_csv_into_dir(csv_name, label, feature):\n",
    "    \"\"\"\n",
    "    split_csv_into_dir takes a csv file and splits its content by the label. The two new data sets pos and neg are split into a train and test set for each. These are now stored in respective directories \n",
    "\n",
    "    :param csv_name: name or url of the csv contining the data\n",
    "    :param label: the label column inside the csv is expected to be bool\n",
    "    :param feature: the feature column inside the csv is expected to be string\n",
    "    \"\"\" \n",
    "\n",
    "    # Read in dataset\n",
    "    data = pd.read_csv(csv_name)\n",
    "    total_data = len(data)\n",
    "    debug(\"---\")\n",
    "    debug(\"Data has been read in\")\n",
    "    debug(\"Columns:\\n\")\n",
    "    debug(data.columns)\n",
    "    debug(\"Head()\\n\")\n",
    "    debug(data.head())\n",
    "    debug(\"Info()\\n\")\n",
    "    debug(data.info())\n",
    "    debug(\"Describe()\\n\")\n",
    "    debug(data.describe())\n",
    "    debug(f\"There are {total_data} entries\")\n",
    "    debug(\"\")\n",
    "    \n",
    "    # Use mask to split list into pos and neg\n",
    "    mask = data[label] == True\n",
    "    pos, neg = data[mask], data[~mask]\n",
    "    debug(\"---\")\n",
    "    debug(\"Data has been split into True and False\\n\")\n",
    "    debug(f\"pos len: {len(pos)}\")\n",
    "    debug(pos.head())\n",
    "    debug(\"\\n-\\n\")\n",
    "    debug(f\"neg len: {len(neg)}\")\n",
    "    debug(neg.head())\n",
    "    debug(\"\")\n",
    "    \n",
    "    # Split pos data and neg into train and test set\n",
    "    data = {'train': {'pos': [],'neg': [],},'test': {'pos': [],'neg': [],}}\n",
    "    data['train']['pos'], data['test']['pos'] = train_test_split(pos, test_size=0.2)\n",
    "    data['train']['neg'], data['test']['neg'] = train_test_split(neg, test_size=0.2)\n",
    "    debug(\"---\")\n",
    "    debug(\"Datasets Have been Split into train and test\")\n",
    "    debug(\"\")    \n",
    "\n",
    "    # Create Folder structure and save files into it \n",
    "    # Code fails if folders already exist\n",
    "    debug(\"---\")\n",
    "    train_test_dirs = ['test', 'train']\n",
    "    pos_neg_dirs = ['pos', 'neg']\n",
    "    for train_test_dir in train_test_dirs:\n",
    "        try:\n",
    "            os.mkdir(train_test_dir)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "\n",
    "        \n",
    "        for pos_neg_dir in pos_neg_dirs:\n",
    "            new_dir = os.path.join(train_test_dir, pos_neg_dir)\n",
    "            try:\n",
    "                os.mkdir(new_dir)\n",
    "            except FileExistsError:\n",
    "                debug(f\"Cleaning Dir: {new_dir}\")\n",
    "                shutil.rmtree(new_dir)\n",
    "                os.mkdir(new_dir)\n",
    "                pass\n",
    "            debug(f\"Dir {new_dir} has been created\")\n",
    "            \n",
    "            data_len = len(data[train_test_dir][pos_neg_dir])\n",
    "            \n",
    "            for idx, row in data[train_test_dir][pos_neg_dir].reset_index(drop=True).iterrows():\n",
    "                #create new file named after current index\n",
    "                with open(os.path.join(new_dir, str(idx)+\".txt\"), \"w\") as file:\n",
    "                    # write the text into the new file\n",
    "                    file.write(row[feature])\n",
    "            \n",
    "            folder_content_len = 0\n",
    "            for path in os.scandir(new_dir):\n",
    "                if path.is_file():\n",
    "                    folder_content_len += 1\n",
    "            if data_len != folder_content_len:\n",
    "                debug(f'Dir {new_dir} is missing Jokes Data: {data_len} Dir Content  {folder_content_len}')\n",
    "            else:\n",
    "                debug(\"All files have been Created!\")\n",
    "    debug(\"Success\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model(tfhub_handle_preprocess, tfhub_handle_encoder, var_layer_dropout, var_activation_funct):\n",
    "    \"\"\"\n",
    "    build_classifier_model returns teh keras model.\n",
    "\n",
    "    :param tfhub_handle_preprocess: \n",
    "    :param tfhub_handle_encoder: \n",
    "    :param var_layer_dropout: \n",
    "    :param var_activation_funct: \n",
    "    :return: Keras Model\n",
    "    \"\"\" \n",
    "    text_input = tf.keras.layers.Input(shape=(),dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    \n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    output = encoder(encoder_inputs)\n",
    "    debug(output)\n",
    "    net = output['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(var_layer_dropout)(net) # HYPERPARAMETER DROPOUT\n",
    "    net = tf.keras.layers.Dense(1, activation=var_activation_funct, name='classifier')(net) # HYPERPARAMETER Layer Size (1) und Activation\n",
    "    \n",
    "    debug(\"Return Model\")\n",
    "    return tf.keras.Model(text_input, net)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_funny(inputs, results):\n",
    "  result_for_printing = [f'{inputs[i]:<30}\\n FUNNY SCORE: {results[i][0]:.6f}\\n' for i in range(len(inputs))]\n",
    "  print(result_for_printing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trained_model(bert_model_name, classifier_model, bucket_name):\n",
    "    \"\"\"\n",
    "    save_trained_model Saves data in folder and create zip \n",
    "\n",
    "    :param bert_model_name: name of the model used\n",
    "    :param classifier_model: classifier model that saves the data\n",
    "    :param bucket_name: Bucket the zip files to upload\n",
    "    \"\"\"\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    dataset_name = \"model_\"+bert_model_name+\"_\"+timestamp\n",
    "    saved_model_path = './{}'.format(dataset_name.replace('/', '_'))\n",
    "    debug(saved_model_path)\n",
    "    debug(f'Saving of {bert_model_name}')\n",
    "    classifier_model.save(saved_model_path, include_optimizer=True)\n",
    "    # todo: add zip folder \n",
    "    file = saved_model_path + '.zip'\n",
    "    zipobj = zipfile.ZipFile(file, 'w', zipfile.ZIP_DEFLATED)\n",
    "    rootlen = len(saved_model_path) + 1\n",
    "    for base, dirs, files in os.walk(saved_model_path):\n",
    "        for file in files:\n",
    "            fn = os.path.join(base, file)\n",
    "            zipobj.write(fn, fn[rootlen:])\n",
    "    # todo: upload to cos\n",
    "    upload_file(file, bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data_frame(df, name, bucket_name):\n",
    "    \"\"\"\n",
    "    export_data_frame Saves dataframe as file\n",
    "\n",
    "    :param df: Data frame\n",
    "    :param name: name of the file to save to\n",
    "    \"\"\"\n",
    "\n",
    "    debug(name)\n",
    "    excel = name + 'xlsx'\n",
    "    csv = name + '.csv'\n",
    "    df.to_excel(excel, sheet_name='sheet_1')\n",
    "    df.to_csv(csv, sep=',')\n",
    "   \n",
    "    # Upload Files To Cos\n",
    "    upload_file(excel, bucket_name)\n",
    "    upload_file(csv, bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the default value should be overridden \n",
    "DESIRED_ACCURACY = 0.93\n",
    "# could need : binary_accuracy instead of accuracy\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('val_binary_accuracy')>DESIRED_ACCURACY): # !!! GEÄNDERT IN VAL_BINARY_ACCURACY für Validation Accuracy !!!\n",
    "            print(f\"\\nReached {logs.get('val_binary_accuracy') * 100}% accuracy Desired {DESIRED_ACCURACY * 100}% so cancelling training!\", )\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Data has been read in\n",
      "Columns:\n",
      "\n",
      "Index(['text', 'humor'], dtype='object')\n",
      "Head()\n",
      "\n",
      "                                                text  humor\n",
      "0  Joe biden rules out 2020 bid: 'guys, i'm not r...  False\n",
      "1  Watch: darvish gave hitter whiplash with slow ...  False\n",
      "2  What do you call a turtle without its shell? d...   True\n",
      "3      5 reasons the 2016 election feels so personal  False\n",
      "4  Pasco police shot mexican migrant from behind,...  False\n",
      "Info()\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    200000 non-null  object\n",
      " 1   humor   200000 non-null  bool  \n",
      "dtypes: bool(1), object(1)\n",
      "memory usage: 1.7+ MB\n",
      "None\n",
      "Describe()\n",
      "\n",
      "                                                     text   humor\n",
      "count                                              200000  200000\n",
      "unique                                             200000       2\n",
      "top     Joe biden rules out 2020 bid: 'guys, i'm not r...   False\n",
      "freq                                                    1  100000\n",
      "There are 200000 entries\n",
      "\n",
      "---\n",
      "Data has been split into True and False\n",
      "\n",
      "pos len: 100000\n",
      "                                                 text  humor\n",
      "2   What do you call a turtle without its shell? d...   True\n",
      "6   What is a pokemon master's favorite kind of pa...   True\n",
      "7   Why do native americans hate it when it rains ...   True\n",
      "9       My family tree is a cactus, we're all pricks.   True\n",
      "13  How are music and candy similar? we throw away...   True\n",
      "\n",
      "-\n",
      "\n",
      "neg len: 100000\n",
      "                                                text  humor\n",
      "0  Joe biden rules out 2020 bid: 'guys, i'm not r...  False\n",
      "1  Watch: darvish gave hitter whiplash with slow ...  False\n",
      "3      5 reasons the 2016 election feels so personal  False\n",
      "4  Pasco police shot mexican migrant from behind,...  False\n",
      "5  Martha stewart tweets hideous food photo, twit...  False\n",
      "\n",
      "---\n",
      "Datasets Have been Split into train and test\n",
      "\n",
      "---\n",
      "Cleaning Dir: test/pos\n",
      "Dir test/pos has been created\n",
      "All files have been Created!\n",
      "Cleaning Dir: test/neg\n",
      "Dir test/neg has been created\n",
      "All files have been Created!\n",
      "Cleaning Dir: train/pos\n",
      "Dir train/pos has been created\n",
      "All files have been Created!\n",
      "Cleaning Dir: train/neg\n",
      "Dir train/neg has been created\n",
      "All files have been Created!\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# todo: change url\n",
    "csv_name = \"https://raw.githubusercontent.com/skywalkeretw/DBE-Humor-Prototype/master/Datasets/dataset.csv\"\n",
    "label = 'humor'\n",
    "feature = 'text'\n",
    "\n",
    "split_csv_into_dir(csv_name, label, feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup COS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new bucket: trained-bert-models-dec-22-hhz-5\n",
      "Bucket: trained-bert-models-dec-22-hhz-5 created!\n"
     ]
    }
   ],
   "source": [
    "bucket_name=\"trained-bert-models-dec-22-hhz-5\"\n",
    "location=\"eu-de-standard\"\n",
    "create_bucket(bucket_name, location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put params in loop / csv\n",
    "var_batch_size = 1024 #\n",
    "var_seed = 42\n",
    "var_layer_dropout = 0.1 \n",
    "var_activation_funct = None # sigmoid, relu\n",
    "var_epochs = 3 # call back code verwenden\n",
    "var_init_lr = 3e-5\n",
    "var_optimizer = 'adamw'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get The Date from the folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw train dataset\n",
      "Found 160000 files belonging to 2 classes.\n",
      "Using 128000 files for training.\n",
      "val data set\n",
      "Found 160000 files belonging to 2 classes.\n",
      "Using 32000 files for validation.\n",
      "test dataset\n",
      "Found 40000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Split into Train validation and test\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = var_batch_size # HYPERPARAMETER\n",
    "seed = var_seed # HYPERPARAMETER\n",
    "debug(\"raw train dataset\")\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "class_names = raw_train_ds.class_names\n",
    "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "debug(\"val data set\")\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "debug(\"test dataset\")\n",
    "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'test',\n",
    "    batch_size=batch_size)\n",
    "\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bert models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readin bert_models as datafram and convert to dict\n"
     ]
    }
   ],
   "source": [
    "#   [{\n",
    "#      \"name\":\"small_bert/bert_en_uncased_L-2_H-128_A-2\",\n",
    "#      \"encoder\":\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1\",\n",
    "#      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "#   },...]\n",
    "debug(\"readin bert_models as datafram and convert to dict\")\n",
    "bert_models = pd.read_json('https://raw.githubusercontent.com/skywalkeretw/DBE-Humor-Prototype/master/Datasets/bert_models.json').to_dict('records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate Models for all enteries in bert_model list\n",
      "start timer\n",
      "name: small_bert/bert_en_uncased_L-2_H-128_A-2 encoder: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1 preprocess: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n",
      "Preprocessing\n",
      "Encoding\n",
      "Build Classifer Model\n",
      "{'encoder_outputs': [<KerasTensor: shape=(None, 128, 128) dtype=float32 (created by layer 'BERT_encoder')>, <KerasTensor: shape=(None, 128, 128) dtype=float32 (created by layer 'BERT_encoder')>], 'sequence_output': <KerasTensor: shape=(None, 128, 128) dtype=float32 (created by layer 'BERT_encoder')>, 'pooled_output': <KerasTensor: shape=(None, 128) dtype=float32 (created by layer 'BERT_encoder')>, 'default': <KerasTensor: shape=(None, 128) dtype=float32 (created by layer 'BERT_encoder')>}\n",
      "Return Model\n",
      "Define The Metrics\n",
      "Create Optimizer\n",
      "Compile Model\n",
      "Training of small_bert/bert_en_uncased_L-2_H-128_A-2\n",
      "Epoch 1/3\n",
      "  3/125 [..............................] - ETA: 15:53 - loss: 0.8977 - binary_accuracy: 0.4482"
     ]
    }
   ],
   "source": [
    "run_metrics = {}\n",
    "\n",
    "debug(\"Generate Models for all enteries in bert_model list\")\n",
    "for model in bert_models:\n",
    "    # start timer\n",
    "    debug(\"start timer\")\n",
    "    st = time.time()\n",
    "\n",
    "    #use data from bert models as parameters\n",
    "    debug(f'name: {model[\"name\"]} encoder: {model[\"encoder\"]} preprocess: {model[\"preprocess\"]}')\n",
    "    bert_model_name = model[\"name\"]\n",
    "    tfhub_handle_encoder = model[\"encoder\"]\n",
    "    tfhub_handle_preprocess = model[\"preprocess\"]\n",
    "\n",
    "    # Preprocessing with Bert\n",
    "    debug(\"Preprocessing\")\n",
    "    bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "    \n",
    "    # Encoding with Bert\n",
    "    debug(\"Encoding\")\n",
    "    bert_model = hub.KerasLayer(tfhub_handle_encoder)\n",
    "    \n",
    "    # Build Classifier Model\n",
    "    debug(\"Build Classifer Model\")\n",
    "    classifier_model = build_classifier_model(tfhub_handle_preprocess,tfhub_handle_encoder,var_layer_dropout,var_activation_funct)\n",
    "    \n",
    "    # Define Metrics\n",
    "    debug(\"Define The Metrics\")\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    metrics = tf.metrics.BinaryAccuracy()\n",
    "\n",
    "    # Init Params\n",
    "    epochs = var_epochs # HYPERPARAMETER\n",
    "    steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "    num_train_steps = steps_per_epoch * epochs\n",
    "    num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "    init_lr = var_init_lr # HYPERPARAMETER\n",
    "    debug(\"Create Optimizer\")\n",
    "    optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                              num_train_steps=num_train_steps,\n",
    "                                              num_warmup_steps=num_warmup_steps,\n",
    "                                              optimizer_type=var_optimizer)\n",
    "\n",
    "    # Compile Model using Params\n",
    "    debug(\"Compile Model\")\n",
    "    classifier_model.compile(optimizer=optimizer,\n",
    "                             loss=loss,\n",
    "                             metrics=metrics)\n",
    "\n",
    "    debug(f'Training of {bert_model_name}')\n",
    "    # Train Model\n",
    "    callbacks = myCallback()\n",
    "    classifier_model.fit(x=train_ds,\n",
    "                            validation_data=val_ds,\n",
    "                            epochs=epochs,\n",
    "                            callbacks=[callbacks])\n",
    "\n",
    "    # Model is evaluated using the test_ds\n",
    "    debug(f\"Evaluate {bert_model_name}\")\n",
    "    loss, accuracy = classifier_model.evaluate(test_ds)\n",
    "\n",
    "    # end timer and calculate duration\n",
    "    duration = time.time() - st\n",
    "    debug(\"End Timer\")\n",
    "    # creatr  metrics file using test_ds\n",
    "    run_metrics[bert_model_name] = {\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy,\n",
    "        'duration': duration\n",
    "    }\n",
    "    \n",
    "    debug(f\"Save Trained Model: {bert_model_name}\")\n",
    "    save_trained_model(bert_model_name, classifier_model, bucket_name)\n",
    "\n",
    "# convert Metrics dict to dataframe and save as excel\n",
    "export_data_frame(pd.DataFrame.from_dict(run_metrics, orient='index'), 'metrics', bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models and Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo rename dataframe\n",
    "ourExamples = pd.read_csv('https://raw.githubusercontent.com/skywalkeretw/DBE-Humor-Prototype/master/Datasets/ourExamples.csv')\n",
    "directories_in_curdir = sorted(list(filter(os.path.isdir, os.listdir(os.curdir))))\n",
    "available_models = [i for i in directories_in_curdir if i.startswith('model_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test using models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in available_models:\n",
    "    \n",
    "    reloaded_model = tf.saved_model.load(model)\n",
    "    reloaded_results = tf.sigmoid(reloaded_model(tf.constant(ourExamples['Sentence'])))\n",
    "\n",
    "    is_funny(ourExamples['Sentence'], reloaded_results)\n",
    "    \n",
    "    ourExamples[model] = reloaded_results\n",
    "debug(ourExamples.head())\n",
    "export_data_frame(ourExamples, 'metrics',bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
