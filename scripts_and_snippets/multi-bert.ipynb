{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning using NLP and bert\n",
    "## Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import zipfile \n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "import ibm_boto3\n",
    "from ibm_botocore.client import Config, ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG=True\n",
    "def debug(d, override = False):\n",
    "    \"\"\"\n",
    "    debug simple mode to enable and disable output\n",
    "\n",
    "    :param d: data that should be displayed\n",
    "    \"\"\" \n",
    "    if DEBUG or override:\n",
    "        print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Constants for IBM COS values\n",
    "COS_ENDPOINT = \"https://\"+\"s3.eu-de.cloud-object-storage.appdomain.cloud\" # Current list avaiable at https://control.cloud-object-storage.cloud.ibm.com/v2/endpoints\n",
    "COS_API_KEY_ID = \"<apikey>\" # eg \"W00YixxxxxxxxxxMB-odB-2ySfTrFBIQQWanc--P3byk\"\n",
    "COS_INSTANCE_CRN = \"<>\" # eg \"crn:v1:bluemix:public:cloud-object-storage:global:a/3bf0d9003xxxxxxxxxx1c3e97696b71c:d6f04d83-6c4f-4a62-a165-696756d63903::\"\n",
    "\n",
    "# Create resource\n",
    "cos_resource = ibm_boto3.resource(\"s3\",\n",
    "    ibm_api_key_id=COS_API_KEY_ID,\n",
    "    ibm_service_instance_id=COS_INSTANCE_CRN,\n",
    "    config=Config(signature_version=\"oauth\"),\n",
    "    endpoint_url=COS_ENDPOINT\n",
    ")\n",
    "\n",
    "# Create client \n",
    "cos_client = ibm_boto3.client(\"s3\",\n",
    "    ibm_api_key_id=COS_API_KEY_ID,\n",
    "    ibm_service_instance_id=COS_INSTANCE_CRN,\n",
    "    config=Config(signature_version=\"oauth\"),\n",
    "    endpoint_url=COS_ENDPOINT\n",
    ")\n",
    "# \"eu-de-standard\"\n",
    "def create_bucket(bucket_name, location):\n",
    "    print(\"Creating new bucket: {0}\".format(bucket_name))\n",
    "    try:\n",
    "        cos_resource.Bucket(bucket_name).create(\n",
    "            CreateBucketConfiguration={\n",
    "                \"LocationConstraint\": location\n",
    "            }\n",
    "        )\n",
    "        print(\"Bucket: {0} created!\".format(bucket_name))\n",
    "    except ClientError as be:\n",
    "        print(\"CLIENT ERROR: {0}\\n\".format(be))\n",
    "    except Exception as e:\n",
    "        print(\"Unable to create bucket: {0}\".format(e))\n",
    "\n",
    "def get_buckets():\n",
    "    print(\"Retrieving list of buckets\")\n",
    "    try:\n",
    "        buckets = cos_resource.buckets.all()\n",
    "        for bucket in buckets:\n",
    "            print(\"Bucket Name: {0}\".format(bucket.name))\n",
    "    except ClientError as be:\n",
    "        print(\"CLIENT ERROR: {0}\\n\".format(be))\n",
    "    except Exception as e:\n",
    "        print(\"Unable to retrieve list buckets: {0}\".format(e))\n",
    "\n",
    "def upload_file(file, bucket_name):\n",
    "    print(\"Uploading {0} to bucket  {1}\".format(file, bucket_name))\n",
    "    try:\n",
    "        key = os.path.basename(file)\n",
    "        cos_client.upload_file(file, bucket_name, key)\n",
    "        print(\"File {0} uploaded to {1}\".format(file, bucket_name))\n",
    "        return key\n",
    "    except ClientError as be:\n",
    "        print(\"CLIENT ERROR: {0}\\n\".format(be))\n",
    "    except Exception as e:\n",
    "        print(\"Unable to upload: {0}\".format(e))\n",
    "\n",
    "def download_file(bucket_name, key, file):\n",
    "    print(\"Downloading {0} from bucket  {1}\".format(key, bucket_name))\n",
    "    try:\n",
    "        cos_client.download_file(Bucket=bucket_name, Key=key,  Filename=file)\n",
    "        print(\"File {0} downloaded from {1}\".format(file, bucket_name))\n",
    "    except ClientError as be:\n",
    "        print(\"CLIENT ERROR: {0}\\n\".format(be))\n",
    "    except Exception as e:\n",
    "        print(\"Unable to download: {0}\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_csv_into_dir(csv_name, label, feature):\n",
    "    \"\"\"\n",
    "    split_csv_into_dir takes a csv file and splits its content by the label. The two new data sets pos and neg are split into a train and test set for each. These are now stored in respective directories \n",
    "\n",
    "    :param csv_name: name or url of the csv contining the data\n",
    "    :param label: the label column inside the csv is expected to be bool\n",
    "    :param feature: the feature column inside the csv is expected to be string\n",
    "    \"\"\" \n",
    "\n",
    "    # Read in dataset\n",
    "    data = pd.read_csv(csv_name)\n",
    "    total_data = len(data)\n",
    "    debug(\"---\")\n",
    "    debug(\"Data has been read in\")\n",
    "    debug(\"Columns:\\n\")\n",
    "    debug(data.columns)\n",
    "    debug(\"Head()\\n\")\n",
    "    debug(data.head())\n",
    "    debug(\"Info()\\n\")\n",
    "    debug(data.info())\n",
    "    debug(\"Describe()\\n\")\n",
    "    debug(data.describe())\n",
    "    debug(f\"There are {total_data} entries\")\n",
    "    debug(\"\")\n",
    "    \n",
    "    # Use mask to split list into pos and neg\n",
    "    mask = data[label] == True\n",
    "    pos, neg = data[mask], data[~mask]\n",
    "    debug(\"---\")\n",
    "    debug(\"Data has been split into True and False\\n\")\n",
    "    debug(f\"pos len: {len(pos)}\")\n",
    "    debug(pos.head())\n",
    "    debug(\"\\n-\\n\")\n",
    "    debug(f\"neg len: {len(neg)}\")\n",
    "    debug(neg.head())\n",
    "    debug(\"\")\n",
    "    \n",
    "    # Split pos data and neg into train and test set\n",
    "    data = {'train': {'pos': [],'neg': [],},'test': {'pos': [],'neg': [],}}\n",
    "    data['train']['pos'], data['test']['pos'] = train_test_split(pos, test_size=0.2)\n",
    "    data['train']['neg'], data['test']['neg'] = train_test_split(neg, test_size=0.2)\n",
    "    debug(\"---\")\n",
    "    debug(\"Datasets Have been Split into train and test\")\n",
    "    debug(\"\")    \n",
    "\n",
    "    # Create Folder structure and save files into it \n",
    "    # Code fails if folders already exist\n",
    "    debug(\"---\")\n",
    "    train_test_dirs = ['test', 'train']\n",
    "    pos_neg_dirs = ['pos', 'neg']\n",
    "    for train_test_dir in train_test_dirs:\n",
    "        try:\n",
    "            os.mkdir(train_test_dir)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "\n",
    "        \n",
    "        for pos_neg_dir in pos_neg_dirs:\n",
    "            new_dir = os.path.join(train_test_dir, pos_neg_dir)\n",
    "            try:\n",
    "                os.mkdir(new_dir)\n",
    "            except FileExistsError:\n",
    "                debug(f\"Cleaning Dir: {new_dir}\")\n",
    "                shutil.rmtree(new_dir)\n",
    "                os.mkdir(new_dir)\n",
    "                pass\n",
    "            debug(f\"Dir {new_dir} has been created\")\n",
    "            \n",
    "            data_len = len(data[train_test_dir][pos_neg_dir])\n",
    "            \n",
    "            for idx, row in data[train_test_dir][pos_neg_dir].reset_index(drop=True).iterrows():\n",
    "                #create new file named after current index\n",
    "                with open(os.path.join(new_dir, str(idx)+\".txt\"), \"w\") as file:\n",
    "                    # write the text into the new file\n",
    "                    file.write(row[feature])\n",
    "            \n",
    "            folder_content_len = 0\n",
    "            for path in os.scandir(new_dir):\n",
    "                if path.is_file():\n",
    "                    folder_content_len += 1\n",
    "            if data_len != folder_content_len:\n",
    "                debug(f'Dir {new_dir} is missing Jokes Data: {data_len} Dir Content  {folder_content_len}')\n",
    "            else:\n",
    "                debug(\"All files have been Created!\")\n",
    "    debug(\"Success\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model(tfhub_handle_preprocess, tfhub_handle_encoder, var_layer_dropout, var_activation_funct):\n",
    "    \"\"\"\n",
    "    build_classifier_model returns teh keras model.\n",
    "\n",
    "    :param tfhub_handle_preprocess: \n",
    "    :param tfhub_handle_encoder: \n",
    "    :param var_layer_dropout: \n",
    "    :param var_activation_funct: \n",
    "    :return: Keras Model\n",
    "    \"\"\" \n",
    "    text_input = tf.keras.layers.Input(shape=(),dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    \n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    output = encoder(encoder_inputs)\n",
    "    debug(output)\n",
    "    net = output['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(var_layer_dropout)(net) # HYPERPARAMETER DROPOUT\n",
    "    net = tf.keras.layers.Dense(1, activation=var_activation_funct, name='classifier')(net) # HYPERPARAMETER Layer Size (1) und Activation\n",
    "    \n",
    "    debug(\"Return Model\")\n",
    "    return tf.keras.Model(text_input, net)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_funny(inputs, results):\n",
    "  result_for_printing = [f'{inputs[i]:<30}\\n FUNNY SCORE: {results[i][0]:.6f}\\n' for i in range(len(inputs))]\n",
    "  print(result_for_printing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trained_model(bert_model_name, classifier_model, bucket_name):\n",
    "    \"\"\"\n",
    "    save_trained_model Saves data in folder and create zip \n",
    "\n",
    "    :param bert_model_name: name of the model used\n",
    "    :param classifier_model: classifier model that saves the data\n",
    "    :param bucket_name: Bucket the zip files to upload\n",
    "    \"\"\"\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    dataset_name = \"model_\"+bert_model_name+\"_\"+timestamp\n",
    "    saved_model_path = './{}'.format(dataset_name.replace('/', '_'))\n",
    "    debug(saved_model_path)\n",
    "    debug(f'Saving of {bert_model_name}')\n",
    "    classifier_model.save(saved_model_path, include_optimizer=True)\n",
    "    # todo: add zip folder \n",
    "    file = saved_model_path + '.zip'\n",
    "    zipobj = zipfile.ZipFile(file, 'w', zipfile.ZIP_DEFLATED)\n",
    "    rootlen = len(saved_model_path) + 1\n",
    "    for base, dirs, files in os.walk(saved_model_path):\n",
    "        for file in files:\n",
    "            fn = os.path.join(base, file)\n",
    "            zipobj.write(fn, fn[rootlen:])\n",
    "    # todo: upload to cos\n",
    "    upload_file(file, bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data_frame(df, name, bucket_name):\n",
    "    \"\"\"\n",
    "    export_data_frame Saves dataframe as file\n",
    "\n",
    "    :param df: Data frame\n",
    "    :param name: name of the file to save to\n",
    "    \"\"\"\n",
    "\n",
    "    debug(name)\n",
    "    excel = name + 'xlsx'\n",
    "    csv = name + '.csv'\n",
    "    df.to_excel(excel, sheet_name='sheet_1')\n",
    "    df.to_csv(csv, sep=',')\n",
    "   \n",
    "    # Upload Files To Cos\n",
    "    upload_file(excel, bucket_name)\n",
    "    upload_file(csv, bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the default value should be overridden \n",
    "DESIRED_ACCURACY = 0.95\n",
    "# could need : binary_accuracy instead of accuracy\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('binary_accuracy')>DESIRED_ACCURACY):\n",
    "            print(f\"\\nReached {logs.get('binary_accuracy') * 100}% accuracy Desired {DESIRED_ACCURACY * 100}% so cancelling training!\", )\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: change url\n",
    "csv_name = \"https://raw.githubusercontent.com/skywalkeretw/DBE-Humor-Prototype/master/Datasets/dataset.csv\"\n",
    "label = 'humor'\n",
    "feature = 'text'\n",
    "\n",
    "split_csv_into_dir(csv_name, label, feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup COS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name=\"trained-bert-models\"\n",
    "location=\"eu-de-standard\"\n",
    "create_bucket(bucket_name, location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put params in loop / csv\n",
    "var_batch_size = 512 #\n",
    "var_seed = 42\n",
    "var_layer_dropout = 0.1 \n",
    "var_activation_funct = None # sigmoid, relu\n",
    "var_epochs = 10 # call back code verwenden\n",
    "var_init_lr = 3e-5\n",
    "var_optimizer = 'adamw'\n",
    "DESIRED_ACCURACY = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get The Date from the folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into Train validation and test\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = var_batch_size # HYPERPARAMETER\n",
    "seed = var_seed # HYPERPARAMETER\n",
    "debug(\"raw train dataset\")\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "class_names = raw_train_ds.class_names\n",
    "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "debug(\"val data set\")\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "debug(\"test dataset\")\n",
    "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'test',\n",
    "    batch_size=batch_size)\n",
    "\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bert models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.8 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#   [{\n",
    "#      \"name\":\"small_bert/bert_en_uncased_L-2_H-128_A-2\",\n",
    "#      \"encoder\":\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1\",\n",
    "#      \"preprocess\":\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "#   },...]\n",
    "debug(\"readin bert_models as datafram and convert to dict\")\n",
    "bert_models = pd.read_json('https://raw.githubusercontent.com/skywalkeretw/DBE-Humor-Prototype/master/Datasets/bert_models.json').to_dict('records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metrics = {}\n",
    "\n",
    "debug(\"Generate Models for all enteries in bert_model list\")\n",
    "for model in bert_models:\n",
    "    # start timer\n",
    "    debug(\"start timer\")\n",
    "    st = time.time()\n",
    "\n",
    "    #use data from bert models as parameters\n",
    "    debug(f'name: {model[\"name\"]} encoder: {model[\"encoder\"]} preprocess: {model[\"preprocess\"]}')\n",
    "    bert_model_name = model[\"name\"]\n",
    "    tfhub_handle_encoder = model[\"encoder\"]\n",
    "    tfhub_handle_preprocess = model[\"preprocess\"]\n",
    "\n",
    "    # Preprocessing with Bert\n",
    "    debug(\"Preprocessing\")\n",
    "    bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "    \n",
    "    # Encoding with Bert\n",
    "    debug(\"Encoding\")\n",
    "    bert_model = hub.KerasLayer(tfhub_handle_encoder)\n",
    "    \n",
    "    # Build Classifier Model\n",
    "    debug(\"Build Classifer Model\")\n",
    "    classifier_model = build_classifier_model(tfhub_handle_preprocess,tfhub_handle_encoder,var_layer_dropout,var_activation_funct)\n",
    "    \n",
    "    # Define Metrics\n",
    "    debug(\"Define The Metrics\")\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    metrics = tf.metrics.BinaryAccuracy()\n",
    "\n",
    "    # Init Params\n",
    "    epochs = var_epochs # HYPERPARAMETER\n",
    "    steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "    num_train_steps = steps_per_epoch * epochs\n",
    "    num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "    init_lr = var_init_lr # HYPERPARAMETER\n",
    "    debug(\"Create Optimizer\")\n",
    "    optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                              num_train_steps=num_train_steps,\n",
    "                                              num_warmup_steps=num_warmup_steps,\n",
    "                                              optimizer_type=var_optimizer)\n",
    "\n",
    "    # Compile Model using Params\n",
    "    debug(\"Compile Model\")\n",
    "    classifier_model.compile(optimizer=optimizer,\n",
    "                             loss=loss,\n",
    "                             metrics=metrics)\n",
    "\n",
    "    debug(f'Training of {bert_model_name}')\n",
    "    # Train Model\n",
    "    callbacks = myCallback()\n",
    "    classifier_model.fit(x=train_ds,\n",
    "                            validation_data=val_ds,\n",
    "                            epochs=epochs,\n",
    "                            callbacks=[callbacks])\n",
    "\n",
    "    # Model is evaluated using the test_ds\n",
    "    debug(f\"Evaluate {bert_model_name}\")\n",
    "    loss, accuracy = classifier_model.evaluate(test_ds)\n",
    "\n",
    "    # end timer and calculate duration\n",
    "    duration = time.time() - st\n",
    "    debug(\"End Timer\")\n",
    "    # creatr  metrics file using test_ds\n",
    "    run_metrics[bert_model_name] = {\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy,\n",
    "        'duration': duration\n",
    "    }\n",
    "    \n",
    "    debug(f\"Save Trained Model: {bert_model_name}\")\n",
    "    save_trained_model(bert_model_name, classifier_model, bucket_name)\n",
    "\n",
    "# convert Metrics dict to dataframe and save as excel\n",
    "export_data_frame(pd.DataFrame.from_dict(run_metrics, orient='index'), 'metrics', bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models and Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo rename dataframe\n",
    "ourExamples = pd.read_csv('https://raw.githubusercontent.com/skywalkeretw/DBE-Humor-Prototype/master/Datasets/ourExamples.csv')\n",
    "directories_in_curdir = sorted(list(filter(os.path.isdir, os.listdir(os.curdir))))\n",
    "available_models = [i for i in directories_in_curdir if i.startswith('model_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test using models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in available_models:\n",
    "    \n",
    "    reloaded_model = tf.saved_model.load(model)\n",
    "    reloaded_results = tf.sigmoid(reloaded_model(tf.constant(ourExamples['Sentence'])))\n",
    "\n",
    "    is_funny(ourExamples['Sentence'], reloaded_results)\n",
    "    \n",
    "    ourExamples[model] = reloaded_results\n",
    "debug(ourExamples.head())\n",
    "export_data_frame(ourExamples, 'metrics',bucket_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header =  ourExamples['Sentence'].to_numpy().tolist()\n",
    "header.insert(0, \"MODEL\")\n",
    "evaluation = pd.DataFrame(columns=header)\n",
    "evaluation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in available_models:\n",
    "    \n",
    "    reloaded_model = tf.saved_model.load(model)\n",
    "    reloaded_results = tf.sigmoid(reloaded_model(tf.constant(ourExamples['Sentence'])))\n",
    "    results = [item for sublist in np.array(reloaded_results.numpy()).tolist() for item in sublist]\n",
    "    row = [model] + results\n",
    "    print(len(row))\n",
    "    evaluation.loc[len(evaluation)] = row\n",
    "\n",
    "evaluation.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.to_excel('bert_models_result.xlsx',index=False)\n",
    "evaluation.to_csv('bert_models_result.csv', sep=',', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
